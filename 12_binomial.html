
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>The Binomial Distribution &#8212; Bite Size Bayes</title>
    
  <link rel="stylesheet" href="_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.e7340bb3dbd8dde6db86f25597f54a1b.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.7d483ff0a819d6edff12ce0b1ead3928.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="The Price Is Right Problem" href="13_price.html" />
    <link rel="prev" title="Comparisons" href="11_faceoff.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  
  <h1 class="site-logo" id="site-title">Bite Size Bayes</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="README.html">
   Bite Size Bayes
  </a>
 </li>
</ul>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="01_linda.html">
   The Laws of Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02_bayes.html">
   Bayes’s Theorem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_cookie.html">
   The Cookie Problem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04_dice.html">
   The Dice Problem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05_test.html">
   Medical Tests
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06_pmf.html">
   Probability Mass Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07_euro.html">
   The Euro Problem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08_soccer.html">
   The World Cup Problem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09_predict.html">
   Prediction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10_joint.html">
   Joint Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="11_faceoff.html">
   Comparisons
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   The Binomial Distribution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="13_price.html">
   <em>
    The Price Is Right
   </em>
   Problem
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/12_binomial.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/AllenDowney/BiteSizeBayes"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>


            <!-- Full screen (wrap in <a> to have style consistency -->
            <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                    data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                    title="Fullscreen mode"><i
                        class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/AllenDowney/BiteSizeBayes/master?urlpath=tree/12_binomial.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i>
            Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-euro-problem-revisited">
   The Euro problem revisited
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#binomial-distribution">
   Binomial distribution
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#estimating-x">
   Estimating x
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evidence">
   Evidence
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#uniformly-distributed-bias">
   Uniformly distributed bias
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayes-factor">
   Bayes factor
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   Exercises
  </a>
 </li>
</ul>

        </nav>
        
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="the-binomial-distribution">
<h1>The Binomial Distribution<a class="headerlink" href="#the-binomial-distribution" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="the-euro-problem-revisited">
<h2>The Euro problem revisited<a class="headerlink" href="#the-euro-problem-revisited" title="Permalink to this headline">¶</a></h2>
<p>In <a class="reference external" href="https://colab.research.google.com/github/AllenDowney/BiteSizeBayes/blob/master/07_euro.ipynb">a previous notebook</a> I presented a problem from David MacKay’s book, <a class="reference external" href="http://www.inference.org.uk/mackay/itila/p0.html"><em>Information Theory, Inference, and Learning Algorithms</em></a>:</p>
<blockquote>
<div><p>A statistical statement appeared in The Guardian on
Friday January 4, 2002:</p>
<blockquote>
<div><p>“When spun on edge 250 times, a Belgian one-euro coin came
up heads 140 times and tails 110. ‘It looks very suspicious
to me’, said Barry Blight, a statistics lecturer at the London
School of Economics. ‘If the coin were unbiased the chance of
getting a result as extreme as that would be less than 7%’.”</p>
</div></blockquote>
<p>But [asks MacKay] do these data give evidence that the coin is biased rather than fair?</p>
</div></blockquote>
<p>To answer this question, we made these modeling decisions:</p>
<ul class="simple">
<li><p>If you spin a coin on edge, there is some probability, <span class="math notranslate nohighlight">\(x\)</span>, that it will land heads up.</p></li>
<li><p>The value of <span class="math notranslate nohighlight">\(x\)</span> varies from one coin to the next, depending on how the coin is balanced and other factors.</p></li>
</ul>
<p>We started with a uniform prior distribution for <span class="math notranslate nohighlight">\(x\)</span>, then updated it 250 times, once for each spin of the coin.  Then we used the posterior distribution to compute the MAP, posterior mean, and a credible interval.</p>
<p>But we never really answered MacKay’s question.</p>
<p>In this notebook, I introduce the binomial distribution and we will use it to solve the Euro problem more efficiently.  Then we’ll get back to MacKay’s question and see if we can find a more satisfying answer.</p>
</div>
<div class="section" id="binomial-distribution">
<h2>Binomial distribution<a class="headerlink" href="#binomial-distribution" title="Permalink to this headline">¶</a></h2>
<p>Suppose I tell you that a coin is “fair”, that is, the probability of heads is 50%.  If you spin it twice, there are four outcomes: <code class="docutils literal notranslate"><span class="pre">HH</span></code>, <code class="docutils literal notranslate"><span class="pre">HT</span></code>, <code class="docutils literal notranslate"><span class="pre">TH</span></code>, and <code class="docutils literal notranslate"><span class="pre">TT</span></code>.</p>
<p>All four outcomes have the same probability, 25%.  If we add up the total number of heads, it is either 0, 1, or 2.  The probability of 0 and 2 is 25%, and the probability of 1 is 50%.</p>
<p>More generally, suppose the probability of heads is <code class="docutils literal notranslate"><span class="pre">p</span></code> and we spin the coin <code class="docutils literal notranslate"><span class="pre">n</span></code> times.  What is the probability that we get a total of <code class="docutils literal notranslate"><span class="pre">k</span></code> heads?</p>
<p>The answer is given by the binomial distribution:</p>
<p><span class="math notranslate nohighlight">\(P(k; n, p) = \binom{n}{k} p^k (1-p)^{n-k}\)</span></p>
<p>where <span class="math notranslate nohighlight">\(\binom{n}{k}\)</span> is the <a class="reference external" href="https://en.wikipedia.org/wiki/Binomial_coefficient">binomial coefficient</a>, usually pronounced “n choose k”.</p>
<p>We can compute this expression ourselves, but we can also use the SciPy function <code class="docutils literal notranslate"><span class="pre">binom.pmf</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">binom</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">p</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">ks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">ks</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
<span class="n">a</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.25, 0.5 , 0.25])
</pre></div>
</div>
</div>
</div>
<p>If we put this result in a Series, the result is the distribution of <code class="docutils literal notranslate"><span class="pre">k</span></code> for the given values of <code class="docutils literal notranslate"><span class="pre">n</span></code> and <code class="docutils literal notranslate"><span class="pre">p</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pmf_k</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">ks</span><span class="p">)</span>
<span class="n">pmf_k</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0    0.25
1    0.50
2    0.25
dtype: float64
</pre></div>
</div>
</div>
</div>
<p>The following function computes the binomial distribution for given values of <code class="docutils literal notranslate"><span class="pre">n</span></code> and <code class="docutils literal notranslate"><span class="pre">p</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">make_binomial</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Make a binomial PMF.</span>
<span class="sd">    </span>
<span class="sd">    n: number of spins</span>
<span class="sd">    p: probability of heads</span>
<span class="sd">    </span>
<span class="sd">    returns: Series representing a PMF</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">a</span> <span class="o">=</span> <span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">ks</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
    <span class="n">pmf_k</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">ks</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pmf_k</span>
</pre></div>
</div>
</div>
</div>
<p>And here’s what it looks like with <code class="docutils literal notranslate"><span class="pre">n=250</span></code> and <code class="docutils literal notranslate"><span class="pre">p=0.5</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pmf_k</span> <span class="o">=</span> <span class="n">make_binomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">pmf_k</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of heads (k)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Probability&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Binomial distribution&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/12_binomial_15_0.png" src="_images/12_binomial_15_0.png" />
</div>
</div>
<p>The most likely value in this distribution is 125:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pmf_k</span><span class="o">.</span><span class="n">idxmax</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>125
</pre></div>
</div>
</div>
</div>
<p>But even though it is the most likely value, the probability that we get exactly 125 heads is only about 5%.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pmf_k</span><span class="p">[</span><span class="mi">125</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.05041221314731537
</pre></div>
</div>
</div>
</div>
<p>In MacKay’s example, we got 140 heads, which is less likely than 125:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pmf_k</span><span class="p">[</span><span class="mi">140</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.008357181724917673
</pre></div>
</div>
</div>
</div>
<p>In the article MacKay quotes, the statistician says, ‘If the coin were unbiased the chance of getting a result as extreme as that would be less than 7%’.</p>
<p>We can use the binomial distribution to check his math.  The following function takes a PMF and computes the total probability of values greater than or equal to <code class="docutils literal notranslate"><span class="pre">threshold</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">prob_ge</span><span class="p">(</span><span class="n">pmf</span><span class="p">,</span> <span class="n">threshold</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Probability of values greater than a threshold.</span>
<span class="sd">    </span>
<span class="sd">    pmf: Series representing a PMF</span>
<span class="sd">    threshold: value to compare to</span>
<span class="sd">    </span>
<span class="sd">    returns: probability</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ge</span> <span class="o">=</span> <span class="p">(</span><span class="n">pmf</span><span class="o">.</span><span class="n">index</span> <span class="o">&gt;=</span> <span class="n">threshold</span><span class="p">)</span>
    <span class="n">total</span> <span class="o">=</span> <span class="n">pmf</span><span class="p">[</span><span class="n">ge</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">total</span>
</pre></div>
</div>
</div>
</div>
<p>Here’s the probability of getting 140 heads or more:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prob_ge</span><span class="p">(</span><span class="n">pmf_k</span><span class="p">,</span> <span class="mi">140</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.033210575620022706
</pre></div>
</div>
</div>
</div>
<p>It’s about 3.3%, which is less than 7%.  The reason is that the statistician includes all values “as extreme as” 140, which includes values less than or equal to 110, because 140 exceeds the expected value by 15 and 110 falls short by 15.</p>
<p>The probability of values less than or equal to 110 is also 3.3%,
so the total probability of values “as extreme” as 140 is about 7%.</p>
<p>The point of this calculation is that these extreme values are unlikely if the coin is fair.</p>
<p>That’s interesting, but it doesn’t answer MacKay’s question.  Let’s see if we can.</p>
</div>
<div class="section" id="estimating-x">
<h2>Estimating x<a class="headerlink" href="#estimating-x" title="Permalink to this headline">¶</a></h2>
<p>As promised, we can use the binomial distribution to solve the Euro problem more efficiently.  Let’s start again with a uniform prior:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">101</span><span class="p">)</span> <span class="o">/</span> <span class="mi">100</span>
<span class="n">uniform</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">xs</span><span class="p">)</span>
<span class="n">uniform</span> <span class="o">/=</span> <span class="n">uniform</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>We can use <code class="docutils literal notranslate"><span class="pre">binom.pmf</span></code> to compute the likelihood of the data for each possible value of <span class="math notranslate nohighlight">\(x\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="mi">140</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">250</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">uniform</span><span class="o">.</span><span class="n">index</span>

<span class="n">likelihood</span> <span class="o">=</span> <span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">xs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now we can do the Bayesian update in the usual way, multiplying the priors and likelihoods,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">posterior</span> <span class="o">=</span> <span class="n">uniform</span> <span class="o">*</span> <span class="n">likelihood</span>
</pre></div>
</div>
</div>
</div>
<p>Computing the total probability of the data,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">total</span> <span class="o">=</span> <span class="n">posterior</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">total</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.003944617569326327
</pre></div>
</div>
</div>
</div>
<p>And normalizing the posterior,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">posterior</span> <span class="o">/=</span> <span class="n">total</span>
</pre></div>
</div>
</div>
</div>
<p>Here’s what it looks like.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">posterior</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Uniform&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Probability of heads (x)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Probability&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Posterior distribution, uniform prior&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x7f53d4f5dd90&gt;
</pre></div>
</div>
<img alt="_images/12_binomial_39_1.png" src="_images/12_binomial_39_1.png" />
</div>
</div>
<p><strong>Exercise:</strong> Based on what we know about coins in the real world, it doesn’t seem like every value of <span class="math notranslate nohighlight">\(x\)</span> is equally likely.  I would expect values near 50% to be more likely and values near the extremes to be less likely.</p>
<p>In Notebook 7, we used a triangle prior to represent this belief about the distribution of <span class="math notranslate nohighlight">\(x\)</span>.  The following code makes a PMF that represents a triangle prior.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ramp_up</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="n">ramp_down</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ramp_up</span><span class="p">,</span> <span class="n">ramp_down</span><span class="p">)</span>

<span class="n">triangle</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">xs</span><span class="p">)</span>
<span class="n">triangle</span> <span class="o">/=</span> <span class="n">triangle</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Update this prior with the likelihoods we just computed and plot the results.</p>
</div>
<div class="section" id="evidence">
<h2>Evidence<a class="headerlink" href="#evidence" title="Permalink to this headline">¶</a></h2>
<p>Finally, let’s get back to MacKay’s question: do these data give evidence that the coin is biased rather than fair?</p>
<p>I’ll use a Bayes table to answer this question, so here’s the function that makes one:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">make_bayes_table</span><span class="p">(</span><span class="n">hypos</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Make a Bayes table.</span>
<span class="sd">    </span>
<span class="sd">    hypos: sequence of hypotheses</span>
<span class="sd">    prior: prior probabilities</span>
<span class="sd">    likelihood: sequence of likelihoods</span>
<span class="sd">    </span>
<span class="sd">    returns: DataFrame</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">table</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="n">hypos</span><span class="p">)</span>
    <span class="n">table</span><span class="p">[</span><span class="s1">&#39;prior&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">prior</span>
    <span class="n">table</span><span class="p">[</span><span class="s1">&#39;likelihood&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">likelihood</span>
    <span class="n">table</span><span class="p">[</span><span class="s1">&#39;unnorm&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">table</span><span class="p">[</span><span class="s1">&#39;prior&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">table</span><span class="p">[</span><span class="s1">&#39;likelihood&#39;</span><span class="p">]</span>
    <span class="n">prob_data</span> <span class="o">=</span> <span class="n">table</span><span class="p">[</span><span class="s1">&#39;unnorm&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">table</span><span class="p">[</span><span class="s1">&#39;posterior&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">table</span><span class="p">[</span><span class="s1">&#39;unnorm&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="n">prob_data</span>
    <span class="k">return</span> <span class="n">table</span>
</pre></div>
</div>
</div>
</div>
<p>Recall that data, <span class="math notranslate nohighlight">\(D\)</span>, is considered evidence in favor of a hypothesis, <code class="docutils literal notranslate"><span class="pre">H</span></code>, if the posterior probability is greater than the prior, that is, if</p>
<p><span class="math notranslate nohighlight">\(P(H|D) &gt; P(H)\)</span></p>
<p>For this example, I’ll call the hypotheses <code class="docutils literal notranslate"><span class="pre">fair</span></code> and <code class="docutils literal notranslate"><span class="pre">biased</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hypos</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;fair&#39;</span><span class="p">,</span> <span class="s1">&#39;biased&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>And just to get started, I’ll assume that the prior probabilities are 50/50.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prior</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Now we have to compute the probability of the data under each hypothesis.</p>
<p>If the coin is fair, the probability of heads is 50%, and we can compute the probability of the data (140 heads out of 250 spins) using the binomial distribution:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="mi">140</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">250</span>

<span class="n">like_fair</span> <span class="o">=</span> <span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">like_fair</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.008357181724917673
</pre></div>
</div>
</div>
</div>
<p>So that’s the probability of the data, given that the coin is fair.</p>
<p>But if the coin is biased, what’s the probability of the data?  Well, that depends on what “biased” means.</p>
<p>If we know ahead of time that “biased” means the probability of heads is 56%, we can use the binomial distribution again:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">like_biased</span> <span class="o">=</span> <span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.56</span><span class="p">)</span>
<span class="n">like_biased</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.05077815959517949
</pre></div>
</div>
</div>
</div>
<p>Now we can put the likelihoods in the Bayes table:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">likes</span> <span class="o">=</span> <span class="p">[</span><span class="n">like_fair</span><span class="p">,</span> <span class="n">like_biased</span><span class="p">]</span>

<span class="n">make_bayes_table</span><span class="p">(</span><span class="n">hypos</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">likes</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>prior</th>
      <th>likelihood</th>
      <th>unnorm</th>
      <th>posterior</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>fair</th>
      <td>0.5</td>
      <td>0.008357</td>
      <td>0.004179</td>
      <td>0.141323</td>
    </tr>
    <tr>
      <th>biased</th>
      <td>0.5</td>
      <td>0.050778</td>
      <td>0.025389</td>
      <td>0.858677</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The posterior probability of <code class="docutils literal notranslate"><span class="pre">biased</span></code> is about 86%, so the data is evidence that the coin is biased, at least for this definition of “biased”.</p>
<p>But we used the data to define the hypothesis, which seems like cheating.  To be fair, we should define “biased” before we see the data.</p>
</div>
<div class="section" id="uniformly-distributed-bias">
<h2>Uniformly distributed bias<a class="headerlink" href="#uniformly-distributed-bias" title="Permalink to this headline">¶</a></h2>
<p>Suppose “biased” means that the probability of heads is anything except 50%, and all other values are equally likely.</p>
<p>We can represent that definition by making a uniform distribution and removing 50%.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">biased_uniform</span> <span class="o">=</span> <span class="n">uniform</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">biased_uniform</span><span class="p">[</span><span class="mi">50</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">biased_uniform</span> <span class="o">/=</span> <span class="n">biased_uniform</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Now, to compute the probability of the data under this hypothesis, we compute the probability of the data for each value of <span class="math notranslate nohighlight">\(x\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xs</span> <span class="o">=</span> <span class="n">biased_uniform</span><span class="o">.</span><span class="n">index</span>
<span class="n">likelihood</span> <span class="o">=</span> <span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">xs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>And then compute the total probability in the usual way:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">like_uniform</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">biased_uniform</span> <span class="o">*</span> <span class="n">likelihood</span><span class="p">)</span>
<span class="n">like_uniform</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.0039004919277704133
</pre></div>
</div>
</div>
</div>
<p>So that’s the probability of the data under the “biased uniform” hypothesis.</p>
<p>Now we make a Bayes table that compares the hypotheses <code class="docutils literal notranslate"><span class="pre">fair</span></code> and <code class="docutils literal notranslate"><span class="pre">biased</span> <span class="pre">uniform</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hypos</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;fair&#39;</span><span class="p">,</span> <span class="s1">&#39;biased uniform&#39;</span><span class="p">]</span>
<span class="n">likes</span> <span class="o">=</span> <span class="p">[</span><span class="n">like_fair</span><span class="p">,</span> <span class="n">like_uniform</span><span class="p">]</span>

<span class="n">make_bayes_table</span><span class="p">(</span><span class="n">hypos</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">likes</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>prior</th>
      <th>likelihood</th>
      <th>unnorm</th>
      <th>posterior</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>fair</th>
      <td>0.5</td>
      <td>0.008357</td>
      <td>0.004179</td>
      <td>0.681792</td>
    </tr>
    <tr>
      <th>biased uniform</th>
      <td>0.5</td>
      <td>0.003900</td>
      <td>0.001950</td>
      <td>0.318208</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Using this definition of <code class="docutils literal notranslate"><span class="pre">biased</span></code>, the posterior is less than the prior, so the data are evidence that the coin is <em>fair</em>.</p>
<p>In this example, the data might support the fair hypothesis or the biased hypothesis, depending on the definition of “biased”.</p>
<p><strong>Exercise:</strong> Suppose “biased” doesn’t mean every value of <span class="math notranslate nohighlight">\(x\)</span> is equally likely.  Maybe values near 50% are more likely and values near the extremes are less likely.  In the previous exercise we created a PMF that represents a triangle-shaped distribution.</p>
<p>We can use it to represent an alternative definition of “biased”:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">biased_triangle</span> <span class="o">=</span> <span class="n">triangle</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">biased_triangle</span><span class="p">[</span><span class="mi">50</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">biased_triangle</span> <span class="o">/=</span> <span class="n">biased_triangle</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Compute the total probability of the data under this definition of “biased” and use a Bayes table to compare it with the fair hypothesis.</p>
<p>Is the data evidence that the coin is biased?</p>
</div>
<div class="section" id="bayes-factor">
<h2>Bayes factor<a class="headerlink" href="#bayes-factor" title="Permalink to this headline">¶</a></h2>
<p>In the previous section, we used a Bayes table to see whether the data are in favor of the fair or biased hypothesis.</p>
<p>I assumed that the prior probabilities were 50/50, but that was an arbitrary choice.</p>
<p>And it was unnecessary, because we don’t really need a Bayes table to say whether the data favor one hypothesis or another: we can just look at the likelihoods.</p>
<p>Under the first definition of biased, <code class="docutils literal notranslate"><span class="pre">x=0.56</span></code>, the likelihood of the biased hypothesis is higher:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">like_fair</span><span class="p">,</span> <span class="n">like_biased</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0.008357181724917673, 0.05077815959517949)
</pre></div>
</div>
</div>
</div>
<p>Under the biased uniform definition, the likelihood of the fair hypothesis is higher.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">like_fair</span><span class="p">,</span> <span class="n">like_uniform</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0.008357181724917673, 0.0039004919277704133)
</pre></div>
</div>
</div>
</div>
<p>The ratio of these likelihoods tells us which hypothesis the data support.</p>
<p>If the ratio is less than 1, the data support the second hypothesis:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">like_fair</span> <span class="o">/</span> <span class="n">like_biased</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.16458220998050985
</pre></div>
</div>
</div>
</div>
<p>If the ratio is greater than 1, the data support the first hypothesis:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">like_fair</span> <span class="o">/</span> <span class="n">like_uniform</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2.142596851801403
</pre></div>
</div>
</div>
</div>
<p>This likelihood ratio is called a <a class="reference external" href="https://en.wikipedia.org/wiki/Bayes_factor">Bayes factor</a>; it provides a concise way to present the strength of a dataset as evidence for or against a hypothesis.</p>
</div>
<div class="section" id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<p>In this notebook I introduced the binomial disrtribution and used it to solve the Euro problem more efficiently.</p>
<p>Then we used the results to (finally) answer the original version of the Euro problem, considering whether the data support the hypothesis that the coin is fair or biased.  We found that the answer depends on how we define “biased”.  And we summarized the results using a Bayes factor, which quantifies the strength of the evidence.</p>
<p><a class="reference external" href="https://colab.research.google.com/github/AllenDowney/BiteSizeBayes/blob/master/13_price.ipynb">In the next notebook</a> we’ll start on a new problem based on the television game show <em>The Price Is Right</em>.</p>
</div>
<div class="section" id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<p><strong>Exercise:</strong> In preparation for an alien invasion, the Earth Defense League has been working on new missiles to shoot down space invaders.  Of course, some missile designs are better than others; let’s assume that each design has some probability of hitting an alien ship, <code class="docutils literal notranslate"><span class="pre">x</span></code>.</p>
<p>Based on previous tests, the distribution of <code class="docutils literal notranslate"><span class="pre">x</span></code> in the population of designs is roughly uniform between 10% and 40%.</p>
<p>Now suppose the new ultra-secret Alien Blaster 9000 is being tested.  In a press conference, a Defense League general reports that the new design has been tested twice, taking two shots during each test.  The results of the test are confidential, so the general won’t say how many targets were hit, but they report: “The same number of targets were hit in the two tests, so we have reason to think this new design is consistent.”</p>
<p>Is this data good or bad; that is, does it increase or decrease your estimate of <code class="docutils literal notranslate"><span class="pre">x</span></code> for the Alien Blaster 9000?</p>
<p>Plot the prior and posterior distributions, and use the following function to compute the prior and posterior means.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">pmf_mean</span><span class="p">(</span><span class="n">pmf</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the mean of a PMF.</span>
<span class="sd">    </span>
<span class="sd">    pmf: Series representing a PMF</span>
<span class="sd">    </span>
<span class="sd">    return: float</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">pmf</span><span class="o">.</span><span class="n">index</span> <span class="o">*</span> <span class="n">pmf</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># With this prior, being &quot;consistent&quot; is more likely</span>
<span class="c1"># to mean &quot;consistently bad&quot;.</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="11_faceoff.html" title="previous page">Comparisons</a>
    <a class='right-next' id="next-link" href="13_price.html" title="next page"><em>The Price Is Right</em> Problem</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Allen B. Downey<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>